{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d02f37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\gunda\\anaconda3\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: requests in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f71229d42646da95439768ebb6ed80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gunda\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gunda\\.cache\\huggingface\\hub\\models--sshleifer--distilbart-cnn-12-6. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6722e0e3ba6d4890ab0b0d78dda7d2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39e6eb191e94ff9871553b70fef1067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2635c0b0502240a681426d2560300778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee43b1c3019541dd96888a1c2706c7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d96f75ecc94c049748997f3c45d054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Error while downloading from https://cdn-lfs.hf.co/sshleifer/distilbart-cnn-12-6/1e46814333b97dfa0f866f58fd15cd7b48ffbe7fd4c1a929caa5f95c7b2fa592?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1740985713&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDk4NTcxM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9zc2hsZWlmZXIvZGlzdGlsYmFydC1jbm4tMTItNi8xZTQ2ODE0MzMzYjk3ZGZhMGY4NjZmNThmZDE1Y2Q3YjQ4ZmZiZTdmZDRjMWE5MjljYWE1Zjk1YzdiMmZhNTkyP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiJ9XX0_&Signature=gV1yFHXNcWr5GGdT%7EwN97-WCZOYLMQBuNZi8aNKGTdgaFjAxmhNfD3B2UBJkRR9qxAs2ipkW9GOSV4R-y%7E0IDIv29AjvHomiyVmiAv96NoshAXnRmdcKIFYn31EQoTW9lqj4z1J1Mh5EtnTtFUHtY6QHzfbz3ALXAJJ%7EWRfJneU7sSL49-qMnnfrq7VOW75eAaU4UmcmlzP-1mf8nxHMXSxJ9g0T7RbyrcbdVnKxFdbM2v2SEJGAo1p6GMgdHpOR4f3O7GGi63JVLjwAHquAuioiatsvdMSul0hsACt2ZhojEU5ssg9S6hxjauvLop5T9QAt%7EpW7bv9-f3XVmcqXAA__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11160d0d640d40d5a4ca69a23327674e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  68%|######7   | 828M/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install transformers\n",
    "from transformers import pipeline\n",
    "summarizer=pipeline(\"summarization\")\n",
    "text=\"\"\"\n",
    "Huggging Face is a company that specializes in natural language processing(NLP).\n",
    "It has developed the Transformers library,which provides state-of-the-art models\n",
    "for a wide range of NLP taska such as text classification,information extraction,\n",
    "question answering,summarization,translation and more.The library is widely used\n",
    "in both academic and industry due to its ease of use and flexibility.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7fa020c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:  The Transformers library provides state-of-the-art models for a wide range of NLP taska such as text classification,information extraction,question answering,summarization,translation and more . The library is widely used in both\n"
     ]
    }
   ],
   "source": [
    "summary=summarizer(text,max_length=50,min_length=20,do_sample=False)\n",
    "print(\"Summary:\",summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8945f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\gunda\\anaconda3\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: torch in c:\\users\\gunda\\anaconda3\\lib\\site-packages (2.6.0+cpu)\n",
      "Requirement already satisfied: filelock in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: requests in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.2.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gunda\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fd496c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8475406b971f42b3971f0d5cbac73ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gunda\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gunda\\.cache\\huggingface\\hub\\models--cardiffnlp--tweet-topic-21-multi. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceae3541c61c42bd8a358aacec891933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b33b93623db408e997d46e6c688ba22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ce37696efd4a9dadacdb1cca5149cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa61159af944bb699402d07737840bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f06b03286a409cacc71c78fd7c8e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.88k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d6a38b79c94bf49b161cfd7c47b188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The latest iPhone was just released with an incredible new camera!\n",
      "Topic: science_&_technology, Confidence: 0.9260\n",
      "\n",
      "Text: Manchester United won their match with a stunning goal in the last minute.NASA just launched a new mission to explore the surface of Mars.\n",
      "Topic: sports_&_esports, Confidence: 0.7513\n",
      "\n",
      "Text: The Oscars had some surprising winners this year!\n",
      "Topic: film_tv_&_video, Confidence: 0.9357\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed2dd85e8c84864915b87ea45201efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification,AutoTokenizer\n",
    "\n",
    "model_name=\"cardiffnlp/tweet-topic-21-multi\"\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "model=AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "labels=[\n",
    "    \"arts_&_culture\",\"business_&_entrepreneurs\",\"celebrity_&_pop_culture\",\"diaries_&_daily_life\",\n",
    "    \"family\",\"fashion_&_style\",\"film_tv_&_video\",\"fitness_&_health\",\"food_&_dining\",\n",
    "    \"gaming\",\"learning_&_educational\",\"music\",\"news_&_social_concern\",\"other_hobbies\",\"relationships\",\n",
    "    \"science_&_technology\",\"sports_&_esports\",\"travel_&_adventure\",\"youth_&_student_life\"\n",
    "]\n",
    "\n",
    "texts=[\n",
    "    \"The latest iPhone was just released with an incredible new camera!\",\n",
    "    \"Manchester United won their match with a stunning goal in the last minute.\"\n",
    "    \"NASA just launched a new mission to explore the surface of Mars.\",\n",
    "    \"The Oscars had some surprising winners this year!\"\n",
    "]\n",
    "\n",
    "inputs=tokenizer(texts,padding=True,truncation=True,return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "      outputs=model(**inputs)\n",
    "\n",
    "probabilities=torch.nn.functional.softmax(outputs.logits,dim=-1)\n",
    "predictions=torch.argmax(probabilities,dim=1)\n",
    "\n",
    "for text,pred,prob in zip(texts,predictions,probabilities):\n",
    "    print(f\"Text: {text}\\nTopic: {labels[pred.item()]}, Confidence: {prob[pred].item():.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a6b6447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:The latest iPhone was just released with an incredible new camera!\n",
      "Topic:science_&_technology,Confidence:0.9260\n",
      "\n",
      "Text:Manchester United won their match with a stunning goal in the last minute.NASA just launched a new mission to explore the surface of Mars.\n",
      "Topic:sports_&_esports,Confidence:0.7513\n",
      "\n",
      "Text:The Oscars had some surprising winners this year!\n",
      "Topic:film_tv_&_video,Confidence:0.9357\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictons=torch.argmax(probabilities,dim=1)\n",
    "for text,pred,prob in zip(texts,predictions,probabilities):\n",
    "    print(f\"Text:{text}\\nTopic:{labels[pred.item()]},Confidence:{prob[pred].item():.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33b22915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a distant galaxy, the galaxy was a vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "prompt = \"Once upon a time in a distant galaxy,\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(**inputs, max_length=50, num_return_sequences=1, temperature=0.7, top_k=50)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3e95c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The latest iPhone was just released with an incredible new camera!\n",
      "Topic: science_&technology, Confidence: 0.9260\n",
      "\n",
      "Text: Manchester United won their match with a stunning goal in the last minute.\n",
      "Topic: sports&esports, Confidence: 0.9989\n",
      "\n",
      "Text: NASA just launched a new mission to explore the surface of Mars.\n",
      "Topic: film_tv&video, Confidence: 0.0052\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "model_name = \"cardiffnlp/tweet-topic-21-multi\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "labels = [\n",
    "    \"arts_&culture\", \"business&entrepreneurs\", \"celebrity&pop_culture\", \"diaries&_daily_life\",\n",
    "    \"family\", \"fashion_&style\", \"film_tv&video\", \"fitness&health\", \"food&_dining\",\n",
    "    \"gaming\", \"learning_&educational\", \"music\", \"news&_social_concern\", \"other_hobbies\",\n",
    "    \"relationships\", \"science_&technology\", \"sports&esports\", \"travel&_adventure\",\n",
    "    \"youth_&_student_life\"\n",
    "]\n",
    "texts = [\n",
    "    \"The latest iPhone was just released with an incredible new camera!\",\n",
    "    \"Manchester United won their match with a stunning goal in the last minute.\",\n",
    "    \"NASA just launched a new mission to explore the surface of Mars.\",\n",
    "    \"The Oscars had some surprising winners this year!\"\n",
    "]\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "for text, pred, prob in zip(texts, predictions, probabilities):\n",
    "    print(f\"Text: {text}\\nTopic: {labels[pred.item()]}, Confidence: {prob[pred].item():.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bf86f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm a language modeler. I'm a programmer. I'm a writer. I'm a writer. I'm a writer. I'm a writer. I'm a writer. I'm a writer. I'm a writer. I\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"gpt2\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Hello, I'm a language model\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "output = model.generate(**inputs, max_length=50, num_return_sequences=1, temperature=0.7, top_k=50)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746f2653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
